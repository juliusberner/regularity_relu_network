# relu_derivative_at_zero
Testing how oft the ReLU derivative gets evaluated at the origin (where it is set to 0) and if this affects the gradient step 
